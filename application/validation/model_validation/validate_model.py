#!/usr/bin/env python3
"""
æ¨¡å‹é©—è­‰è…³æœ¬
ç”¨æ–¼é©—è­‰æ–°è¨“ç·´çš„æ¨¡å‹æ˜¯å¦æ»¿è¶³éƒ¨ç½²æ¨™æº–
"""

import sys
import argparse
import json
import joblib
from pathlib import Path
from datetime import datetime
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent.parent.parent
sys.path.append(str(project_root))

class ModelValidator:
    """æ¨¡å‹é©—è­‰å™¨"""

    def __init__(self, accuracy_threshold=0.9, performance_threshold_ms=100):
        self.accuracy_threshold = accuracy_threshold
        self.performance_threshold_ms = performance_threshold_ms
        self.validation_results = {}

    def load_latest_model(self, model_registry_path):
        """è¼‰å…¥æœ€æ–°çš„æ¨¡å‹"""
        registry_path = Path(model_registry_path)

        if not registry_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹è¨»å†Šè¡¨ç›®éŒ„ä¸å­˜åœ¨: {registry_path}")

        # å°‹æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
        model_files = list(registry_path.glob("*.joblib"))
        if not model_files:
            raise FileNotFoundError(f"åœ¨ {registry_path} ä¸­æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")

        # æŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼Œç²å–æœ€æ–°çš„
        latest_model_file = max(model_files, key=lambda x: x.stat().st_mtime)

        print(f"ğŸ“¦ è¼‰å…¥æ¨¡å‹: {latest_model_file}")
        model = joblib.load(latest_model_file)

        # å˜—è©¦è¼‰å…¥å°æ‡‰çš„æŒ‡æ¨™æ–‡ä»¶
        metrics_file = latest_model_file.with_suffix('.json')
        metrics = None
        if metrics_file.exists():
            with open(metrics_file, 'r') as f:
                metrics = json.load(f)

        return model, metrics, latest_model_file

    def validate_accuracy(self, model):
        """é©—è­‰æ¨¡å‹æº–ç¢ºç‡"""
        print("ğŸ¯ é©—è­‰æ¨¡å‹æº–ç¢ºç‡...")

        # è¼‰å…¥æ¸¬è©¦è³‡æ–™
        iris = load_iris()
        X, y = iris.data, iris.target

        # ä½¿ç”¨ä¸åŒçš„éš¨æ©Ÿç¨®å­å‰µå»ºé©—è­‰é›†
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.3, random_state=123, stratify=y
        )

        # é æ¸¬
        y_pred = model.predict(X_val)
        accuracy = accuracy_score(y_val, y_pred)

        # ç”Ÿæˆåˆ†é¡å ±å‘Š
        report = classification_report(
            y_val, y_pred,
            target_names=iris.target_names,
            output_dict=True
        )

        validation_result = {
            "accuracy": accuracy,
            "threshold": self.accuracy_threshold,
            "passed": accuracy >= self.accuracy_threshold,
            "classification_report": report
        }

        self.validation_results["accuracy_validation"] = validation_result

        print(f"æº–ç¢ºç‡: {accuracy:.4f} (é–¾å€¼: {self.accuracy_threshold})")
        print(f"çµæœ: {'âœ… é€šé' if validation_result['passed'] else 'âŒ æœªé€šé'}")

        return validation_result

    def validate_performance(self, model, num_samples=1000):
        """é©—è­‰æ¨¡å‹æ¨è«–æ€§èƒ½"""
        print("âš¡ é©—è­‰æ¨¡å‹æ¨è«–æ€§èƒ½...")

        # ç”Ÿæˆæ¸¬è©¦è³‡æ–™
        X_test = np.random.rand(num_samples, 4)

        # æ¸¬é‡æ¨è«–æ™‚é–“
        import time

        start_time = time.time()
        predictions = model.predict(X_test)
        end_time = time.time()

        total_time_ms = (end_time - start_time) * 1000
        avg_time_per_sample_ms = total_time_ms / num_samples

        validation_result = {
            "total_time_ms": total_time_ms,
            "avg_time_per_sample_ms": avg_time_per_sample_ms,
            "samples_tested": num_samples,
            "threshold_ms": self.performance_threshold_ms,
            "passed": avg_time_per_sample_ms <= self.performance_threshold_ms
        }

        self.validation_results["performance_validation"] = validation_result

        print(f"å¹³å‡æ¨è«–æ™‚é–“: {avg_time_per_sample_ms:.4f} ms/æ¨£æœ¬ (é–¾å€¼: {self.performance_threshold_ms} ms)")
        print(f"çµæœ: {'âœ… é€šé' if validation_result['passed'] else 'âŒ æœªé€šé'}")

        return validation_result

    def validate_model_structure(self, model):
        """é©—è­‰æ¨¡å‹çµæ§‹"""
        print("ğŸ—ï¸ é©—è­‰æ¨¡å‹çµæ§‹...")

        validation_result = {
            "model_type": type(model).__name__,
            "has_predict_method": hasattr(model, 'predict'),
            "has_predict_proba_method": hasattr(model, 'predict_proba'),
            "feature_importances_available": hasattr(model, 'feature_importances_')
        }

        # æª¢æŸ¥å¿…è¦çš„æ–¹æ³•
        required_methods = ['predict', 'predict_proba']
        missing_methods = [method for method in required_methods if not hasattr(model, method)]

        validation_result["missing_methods"] = missing_methods
        validation_result["passed"] = len(missing_methods) == 0

        self.validation_results["structure_validation"] = validation_result

        print(f"æ¨¡å‹é¡å‹: {validation_result['model_type']}")
        print(f"å¿…è¦æ–¹æ³•æª¢æŸ¥: {'âœ… é€šé' if validation_result['passed'] else 'âŒ æœªé€šé'}")
        if missing_methods:
            print(f"ç¼ºå°‘æ–¹æ³•: {missing_methods}")

        return validation_result

    def validate_input_output(self, model):
        """é©—è­‰è¼¸å…¥è¼¸å‡ºæ ¼å¼"""
        print("ğŸ”„ é©—è­‰è¼¸å…¥è¼¸å‡ºæ ¼å¼...")

        validation_result = {"passed": True, "errors": []}

        try:
            # æ¸¬è©¦æ­£å¸¸è¼¸å…¥
            test_input = np.array([[5.1, 3.5, 1.4, 0.2]])
            prediction = model.predict(test_input)
            probabilities = model.predict_proba(test_input)

            # é©—è­‰è¼¸å‡ºæ ¼å¼
            if not isinstance(prediction, np.ndarray):
                validation_result["errors"].append("predict() æ‡‰è¿”å› numpy array")
                validation_result["passed"] = False

            if len(prediction) != 1:
                validation_result["errors"].append("é æ¸¬çµæœæ•¸é‡æ‡‰èˆ‡è¼¸å…¥æ¨£æœ¬æ•¸ç›¸ç­‰")
                validation_result["passed"] = False

            if not isinstance(probabilities, np.ndarray):
                validation_result["errors"].append("predict_proba() æ‡‰è¿”å› numpy array")
                validation_result["passed"] = False

            if probabilities.shape != (1, 3):  # 1 æ¨£æœ¬, 3 é¡åˆ¥
                validation_result["errors"].append("æ¦‚ç‡è¼¸å‡ºå½¢ç‹€ä¸æ­£ç¢º")
                validation_result["passed"] = False

            # æ¸¬è©¦é‚Šç•Œæƒ…æ³
            extreme_input = np.array([[0.0, 0.0, 0.0, 0.0]])
            model.predict(extreme_input)
            model.predict_proba(extreme_input)

            validation_result.update({
                "prediction_shape": prediction.shape,
                "probabilities_shape": probabilities.shape,
                "prediction_dtype": str(prediction.dtype),
                "probabilities_dtype": str(probabilities.dtype)
            })

        except Exception as e:
            validation_result["errors"].append(f"è¼¸å…¥è¼¸å‡ºæ¸¬è©¦å¤±æ•—: {str(e)}")
            validation_result["passed"] = False

        self.validation_results["input_output_validation"] = validation_result

        print(f"çµæœ: {'âœ… é€šé' if validation_result['passed'] else 'âŒ æœªé€šé'}")
        if validation_result["errors"]:
            for error in validation_result["errors"]:
                print(f"  éŒ¯èª¤: {error}")

        return validation_result

    def run_full_validation(self, model_registry_path):
        """é‹è¡Œå®Œæ•´çš„æ¨¡å‹é©—è­‰"""
        print("ğŸš€ é–‹å§‹æ¨¡å‹é©—è­‰æµç¨‹...")
        print("=" * 60)

        try:
            # è¼‰å…¥æ¨¡å‹
            model, metrics, model_file = self.load_latest_model(model_registry_path)

            # é‹è¡Œå„é …é©—è­‰
            accuracy_result = self.validate_accuracy(model)
            performance_result = self.validate_performance(model)
            structure_result = self.validate_model_structure(model)
            io_result = self.validate_input_output(model)

            # è¨ˆç®—æ•´é«”çµæœ
            all_validations = [
                accuracy_result["passed"],
                performance_result["passed"],
                structure_result["passed"],
                io_result["passed"]
            ]

            overall_passed = all(all_validations)

            # æº–å‚™æœ€çµ‚å ±å‘Š
            validation_summary = {
                "model_file": str(model_file),
                "validation_timestamp": datetime.now().isoformat(),
                "overall_passed": overall_passed,
                "validations": self.validation_results,
                "summary": {
                    "accuracy_passed": accuracy_result["passed"],
                    "performance_passed": performance_result["passed"],
                    "structure_passed": structure_result["passed"],
                    "input_output_passed": io_result["passed"]
                }
            }

            # ä¿å­˜é©—è­‰å ±å‘Š
            report_path = model_file.parent / f"validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_path, 'w') as f:
                json.dump(validation_summary, f, indent=2, default=str)

            print("=" * 60)
            print("ğŸ“‹ é©—è­‰ç¸½çµ:")
            print(f"æ•´é«”çµæœ: {'âœ… é€šéæ‰€æœ‰é©—è­‰' if overall_passed else 'âŒ å­˜åœ¨é©—è­‰å¤±æ•—'}")
            print(f"é©—è­‰å ±å‘Š: {report_path}")

            return overall_passed, validation_summary

        except Exception as e:
            print(f"âŒ é©—è­‰éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return False, {"error": str(e)}

def main():
    parser = argparse.ArgumentParser(description="æ¨¡å‹é©—è­‰å·¥å…·")
    parser.add_argument("--model-path", type=str, required=True,
                      help="æ¨¡å‹è¨»å†Šè¡¨ç›®éŒ„è·¯å¾‘")
    parser.add_argument("--threshold", type=float, default=0.9,
                      help="æº–ç¢ºç‡é–¾å€¼ (é è¨­: 0.9)")
    parser.add_argument("--performance-threshold", type=float, default=100,
                      help="æ€§èƒ½é–¾å€¼ ms (é è¨­: 100)")

    args = parser.parse_args()

    # å‰µå»ºé©—è­‰å™¨ä¸¦é‹è¡Œé©—è­‰
    validator = ModelValidator(
        accuracy_threshold=args.threshold,
        performance_threshold_ms=args.performance_threshold
    )

    passed, results = validator.run_full_validation(args.model_path)

    # æ ¹æ“šé©—è­‰çµæœè¨­ç½®é€€å‡ºç¢¼
    sys.exit(0 if passed else 1)

if __name__ == "__main__":
    main()