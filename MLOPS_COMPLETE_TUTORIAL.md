# ğŸš€ MLOps è‡ªå‹•åŒ–å…¨å¥—æ•™å­¸ - å®Œæ•´å¯¦æˆ°æŒ‡å—

## ğŸ“– æ•™å­¸ç›®æ¨™

é€šéæœ¬æ•™å­¸ï¼Œæ‚¨å°‡å­¸æœƒï¼š
- å»ºç«‹ç«¯åˆ°ç«¯çš„ MLOps æµæ°´ç·š
- å¯¦ç¾æ¨¡å‹è‡ªå‹•è¨“ç·´ã€é©—è­‰ã€éƒ¨ç½²
- è¨­ç½®ç›£æ§å’Œè‡ªå‹•åŒ– CI/CD
- æŒæ¡ç”Ÿç”¢ç´šæ©Ÿå™¨å­¸ç¿’ç³»çµ±æ¶æ§‹

## ğŸ¯ å­¸ç¿’è·¯å¾‘

```mermaid
graph LR
    A[ç’°å¢ƒè¨­ç½®] --> B[æ¨¡å‹é–‹ç™¼]
    B --> C[å¯¦é©—ç®¡ç†]
    C --> D[è‡ªå‹•è¨“ç·´]
    D --> E[æ¨¡å‹é©—è­‰]
    E --> F[æœå‹™éƒ¨ç½²]
    F --> G[ç›£æ§é‹ç¶­]
    G --> H[æŒçºŒå„ªåŒ–]
```

---

# ç¬¬ä¸€ç« ï¼šç’°å¢ƒè¨­ç½®èˆ‡åŸºç¤é…ç½®

## ğŸ› ï¸ 1.1 é–‹ç™¼ç’°å¢ƒæº–å‚™

### æ­¥é©Ÿ 1ï¼šä¸€éµè¨­ç½®å°ˆæ¡ˆ (æ¨è–¦)
```bash
# è¤‡è£½å°ˆæ¡ˆ
git clone <your-repo>
cd mlops-template

# ä¸€éµåŸ·è¡Œå®Œæ•´è¨­ç½® (åŒ…å«æ‰€æœ‰ä¿®å¾©)
bash scripts/quickstart.sh

# æˆ–è€…åªå®‰è£ä¾è³´
bash scripts/quickstart.sh --install-only
```

### æ­¥é©Ÿ 2ï¼šæ‰‹å‹•è¨­ç½® (å¦‚éœ€è‡ªå®šç¾©)
```bash
# æª¢æŸ¥ Poetry æ˜¯å¦å®‰è£
poetry --version

# ä½¿ç”¨å°ˆé–€çš„ Poetry è¨­ç½®è…³æœ¬
bash scripts/setup/setup_poetry.sh

# æˆ–æ‰‹å‹•å®‰è£ä¾è³´
make install    # æ¨è–¦ï¼šå®Œæ•´å®‰è£
make install-dev  # æˆ–ï¼šæœ€å°åŒ–å®‰è£ (åƒ…é–‹ç™¼å·¥å…·)

# é©—è­‰ GPU è¨­ç½®
make checkgpu
```

### æ­¥é©Ÿ 3ï¼šé–‹ç™¼å®¹å™¨è¨­ç½® (å¯é¸)
```bash
# VS Code ç”¨æˆ¶
# 1. æ‰“é–‹ Command Palette (Ctrl+Shift+P)
# 2. é¸æ“‡ "Dev Containers: Reopen in Container"
# 3. ç­‰å¾…å®¹å™¨æ§‹å»ºå®Œæˆ

# é©—è­‰ç’°å¢ƒ
poetry shell
python --version
nvidia-smi
```

### æ­¥é©Ÿ 3ï¼šé …ç›®çµæ§‹ç†è§£
```
mlops-template/
â”œâ”€â”€ domain/              # é ˜åŸŸå±¤ï¼šæ ¸å¿ƒæ¥­å‹™é‚è¼¯
â”‚   â”œâ”€â”€ data/           # è³‡æ–™ç®¡ç†
â”‚   â”œâ”€â”€ models/         # æ¨¡å‹é–‹ç™¼
â”‚   â””â”€â”€ experiments/    # å¯¦é©—è¨˜éŒ„
â”œâ”€â”€ application/         # æ‡‰ç”¨å±¤ï¼šæ¥­å‹™æ‡‰ç”¨
â”‚   â”œâ”€â”€ training/       # è¨“ç·´æµæ°´ç·š
â”‚   â”œâ”€â”€ inference/      # æ¨è«–æœå‹™
â”‚   â””â”€â”€ validation/     # æ¨¡å‹é©—è­‰
â”œâ”€â”€ infrastructure/     # åŸºç¤è¨­æ–½å±¤
â”‚   â”œâ”€â”€ deployment/     # éƒ¨ç½²é…ç½®
â”‚   â”œâ”€â”€ monitoring/     # ç›£æ§è¨­ç½®
â”‚   â””â”€â”€ cicd/          # CI/CD é…ç½®
â””â”€â”€ shared/            # å…±äº«å·¥å…·èˆ‡é…ç½®
```

---

# ç¬¬äºŒç« ï¼šæ¨¡å‹é–‹ç™¼èˆ‡å¯¦é©—ç®¡ç†

## ğŸ”¬ 2.1 å»ºç«‹æ‚¨çš„ç¬¬ä¸€å€‹ ML å¯¦é©—

### å¯¦æˆ°ç·´ç¿’ï¼šIris åˆ†é¡æ¨¡å‹

#### æ­¥é©Ÿ 1ï¼šæ•¸æ“šæº–å‚™
```bash
# é€²å…¥å¯¦é©—ç›®éŒ„
cd domain/experiments/notebooks

# å•Ÿå‹• Jupyter Lab
poetry run jupyter lab
```

åœ¨ Jupyter ä¸­å‰µå»ºæ–°çš„ notebookï¼š`iris_experiment.ipynb`

```python
# æ•¸æ“šæº–å‚™å’Œæ¢ç´¢æ€§åˆ†æ
import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# è¼‰å…¥æ•¸æ“š
iris = load_iris()
X, y = iris.data, iris.target

# å‰µå»º DataFrame
df = pd.DataFrame(X, columns=iris.feature_names)
df['target'] = y
df['target_name'] = df['target'].map({i: name for i, name in enumerate(iris.target_names)})

# æ•¸æ“šæ¢ç´¢
print("æ•¸æ“šé›†æ¦‚è¦½:")
print(df.head())
print(f"æ•¸æ“šå½¢ç‹€: {df.shape}")

# å¯è¦–åŒ–
plt.figure(figsize=(12, 8))
sns.pairplot(df, hue='target_name')
plt.title("Iris Dataset Pairplot")
plt.show()
```

#### æ­¥é©Ÿ 2ï¼šæ¨¡å‹è¨“ç·´å’Œè©•ä¼°
```python
# åˆ†å‰²æ•¸æ“š
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# è¨“ç·´æ¨¡å‹
models = {
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'LogisticRegression': LogisticRegression(random_state=42),
    'SVC': SVC(random_state=42)
}

results = {}
for name, model in models.items():
    # è¨“ç·´
    model.fit(X_train, y_train)

    # é æ¸¬
    y_pred = model.predict(X_test)

    # è©•ä¼°
    accuracy = accuracy_score(y_test, y_pred)
    results[name] = {
        'model': model,
        'accuracy': accuracy,
        'predictions': y_pred
    }

    print(f"{name} æº–ç¢ºç‡: {accuracy:.4f}")
    print(f"{name} åˆ†é¡å ±å‘Š:")
    print(classification_report(y_test, y_pred, target_names=iris.target_names))
    print("-" * 50)
```

### æ­¥é©Ÿ 3ï¼šå¯¦é©—è¨˜éŒ„å’Œç‰ˆæœ¬ç®¡ç†
```python
import mlflow
import mlflow.sklearn
from datetime import datetime

# è¨­ç½® MLflow
mlflow.set_experiment("iris_classification")

# è¨˜éŒ„æœ€ä½³æ¨¡å‹
best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])
best_model = results[best_model_name]['model']
best_accuracy = results[best_model_name]['accuracy']

with mlflow.start_run(run_name=f"iris_{best_model_name}_{datetime.now().strftime('%Y%m%d_%H%M')}"):
    # è¨˜éŒ„åƒæ•¸
    mlflow.log_param("model_type", best_model_name)
    mlflow.log_param("test_size", 0.2)
    mlflow.log_param("random_state", 42)

    # è¨˜éŒ„æŒ‡æ¨™
    mlflow.log_metric("accuracy", best_accuracy)

    # è¨˜éŒ„æ¨¡å‹
    mlflow.sklearn.log_model(
        sk_model=best_model,
        artifact_path="model",
        registered_model_name="iris_classifier"
    )

    print(f"æ¨¡å‹ {best_model_name} å·²è¨˜éŒ„ï¼Œæº–ç¢ºç‡: {best_accuracy:.4f}")
```

## ğŸ¯ 2.2 å»ºç«‹å¯é‡è¤‡çš„è¨“ç·´æµæ°´ç·š

### å‰µå»ºçµæ§‹åŒ–çš„è¨“ç·´è…³æœ¬

å‰µå»º `application/training/pipelines/iris_training_pipeline.py`ï¼š

```python
import os
import sys
import argparse
import json
from pathlib import Path
from datetime import datetime
import joblib

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent.parent.parent
sys.path.append(str(project_root))

import mlflow
import mlflow.sklearn
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np

class IrisTrainingPipeline:
    """Iris åˆ†é¡æ¨¡å‹è¨“ç·´æµæ°´ç·š"""

    def __init__(self, config_path=None):
        self.config = self.load_config(config_path)
        self.setup_mlflow()

    def load_config(self, config_path):
        """è¼‰å…¥è¨“ç·´é…ç½®"""
        default_config = {
            "model": {
                "n_estimators": 100,
                "max_depth": None,
                "random_state": 42
            },
            "data": {
                "test_size": 0.2,
                "random_state": 42
            },
            "experiment_name": "iris_classification_pipeline"
        }

        if config_path and Path(config_path).exists():
            with open(config_path, 'r') as f:
                config = json.load(f)
            # åˆä½µé…ç½®
            for key in default_config:
                if key not in config:
                    config[key] = default_config[key]
            return config

        return default_config

    def setup_mlflow(self):
        """è¨­ç½® MLflow"""
        mlflow.set_experiment(self.config["experiment_name"])

    def load_data(self):
        """è¼‰å…¥å’Œæº–å‚™æ•¸æ“š"""
        print("ğŸ“Š è¼‰å…¥æ•¸æ“š...")
        iris = load_iris()
        X, y = iris.data, iris.target

        X_train, X_test, y_train, y_test = train_test_split(
            X, y,
            test_size=self.config["data"]["test_size"],
            random_state=self.config["data"]["random_state"],
            stratify=y
        )

        print(f"è¨“ç·´é›†å¤§å°: {X_train.shape}")
        print(f"æ¸¬è©¦é›†å¤§å°: {X_test.shape}")

        return X_train, X_test, y_train, y_test, iris.target_names

    def train_model(self, X_train, y_train):
        """è¨“ç·´æ¨¡å‹"""
        print("ğŸ”§ è¨“ç·´æ¨¡å‹...")
        model = RandomForestClassifier(**self.config["model"])
        model.fit(X_train, y_train)
        return model

    def evaluate_model(self, model, X_test, y_test, target_names):
        """è©•ä¼°æ¨¡å‹"""
        print("ğŸ“ˆ è©•ä¼°æ¨¡å‹...")
        y_pred = model.predict(X_test)

        accuracy = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)
        cm = confusion_matrix(y_test, y_pred)

        return {
            'accuracy': accuracy,
            'classification_report': report,
            'confusion_matrix': cm,
            'predictions': y_pred
        }

    def save_model(self, model, metrics, model_name="iris_classifier"):
        """å„²å­˜æ¨¡å‹å’ŒæŒ‡æ¨™"""
        print("ğŸ’¾ å„²å­˜æ¨¡å‹...")

        # å‰µå»ºæ¨¡å‹ç›®éŒ„
        model_dir = project_root / "application" / "registry" / "model_registry"
        model_dir.mkdir(parents=True, exist_ok=True)

        # å„²å­˜æ¨¡å‹
        model_path = model_dir / f"{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib"
        joblib.dump(model, model_path)

        # å„²å­˜æŒ‡æ¨™
        metrics_path = model_path.with_suffix('.json')
        with open(metrics_path, 'w') as f:
            # å°‡ numpy æ•¸çµ„è½‰æ›ç‚ºåˆ—è¡¨
            serializable_metrics = {}
            for key, value in metrics.items():
                if isinstance(value, np.ndarray):
                    serializable_metrics[key] = value.tolist()
                else:
                    serializable_metrics[key] = value
            json.dump(serializable_metrics, f, indent=2)

        return str(model_path), str(metrics_path)

    def run_training(self):
        """åŸ·è¡Œå®Œæ•´çš„è¨“ç·´æµæ°´ç·š"""
        print("ğŸš€ é–‹å§‹ MLOps è¨“ç·´æµæ°´ç·š...")

        with mlflow.start_run(run_name=f"iris_pipeline_{datetime.now().strftime('%Y%m%d_%H%M')}"):
            # è¼‰å…¥æ•¸æ“š
            X_train, X_test, y_train, y_test, target_names = self.load_data()

            # è¨˜éŒ„åƒæ•¸
            mlflow.log_params(self.config["model"])
            mlflow.log_params(self.config["data"])

            # è¨“ç·´æ¨¡å‹
            model = self.train_model(X_train, y_train)

            # è©•ä¼°æ¨¡å‹
            metrics = self.evaluate_model(model, X_test, y_test, target_names)

            # è¨˜éŒ„æŒ‡æ¨™
            mlflow.log_metric("accuracy", metrics['accuracy'])
            mlflow.log_metric("precision_macro", metrics['classification_report']['macro avg']['precision'])
            mlflow.log_metric("recall_macro", metrics['classification_report']['macro avg']['recall'])
            mlflow.log_metric("f1_macro", metrics['classification_report']['macro avg']['f1-score'])

            # è¨˜éŒ„æ¨¡å‹åˆ° MLflow
            mlflow.sklearn.log_model(
                sk_model=model,
                artifact_path="model",
                registered_model_name="iris_classifier_pipeline"
            )

            # å„²å­˜æ¨¡å‹åˆ°æœ¬åœ°
            model_path, metrics_path = self.save_model(model, metrics)

            print("âœ… è¨“ç·´å®Œæˆï¼")
            print(f"ğŸ“Š æº–ç¢ºç‡: {metrics['accuracy']:.4f}")
            print(f"ğŸ’¾ æ¨¡å‹å„²å­˜ä½ç½®: {model_path}")
            print(f"ğŸ“ˆ æŒ‡æ¨™å„²å­˜ä½ç½®: {metrics_path}")

            return model, metrics

def main():
    parser = argparse.ArgumentParser(description="Iris åˆ†é¡æ¨¡å‹è¨“ç·´æµæ°´ç·š")
    parser.add_argument("--config", type=str, help="è¨“ç·´é…ç½®æ–‡ä»¶è·¯å¾‘")
    args = parser.parse_args()

    # åŸ·è¡Œè¨“ç·´æµæ°´ç·š
    pipeline = IrisTrainingPipeline(config_path=args.config)
    model, metrics = pipeline.run_training()

    return model, metrics

if __name__ == "__main__":
    main()
```

### å‰µå»ºè¨“ç·´é…ç½®æ–‡ä»¶

å‰µå»º `application/training/configs/iris_config.json`ï¼š

```json
{
  "model": {
    "n_estimators": 150,
    "max_depth": 10,
    "min_samples_split": 2,
    "min_samples_leaf": 1,
    "random_state": 42
  },
  "data": {
    "test_size": 0.2,
    "random_state": 42
  },
  "experiment_name": "iris_production_pipeline",
  "model_name": "iris_rf_classifier",
  "version": "1.0.0"
}
```

---

# ç¬¬ä¸‰ç« ï¼šæ¨¡å‹æœå‹™åŒ–èˆ‡ API é–‹ç™¼

## ğŸš€ 3.1 ä½¿ç”¨ BentoML å»ºç«‹æ¨è«–æœå‹™

### æ­¥é©Ÿ 1ï¼šå»ºç«‹ BentoML æœå‹™

å‰µå»º `application/inference/services/iris_service.py`ï¼š

```python
import numpy as np
import pandas as pd
from typing import List
import bentoml
from bentoml.io import NumpyNdarray, JSON
from pydantic import BaseModel

# å®šç¾©è¼¸å…¥æ•¸æ“šæ¨¡å‹
class IrisFeatures(BaseModel):
    sepal_length: float
    sepal_width: float
    petal_length: float
    petal_width: float

class IrisBatch(BaseModel):
    instances: List[IrisFeatures]

# è¼‰å…¥å·²è¨“ç·´çš„æ¨¡å‹ (ä½¿ç”¨æ–°çš„ runner æ¨¡å¼)
iris_model_runner = bentoml.sklearn.get("iris_classifier_pipeline:latest").to_runner()

@bentoml.Service(
    name="iris_classifier",
    runners=[iris_model_runner],
)
class IrisClassifier:
    """Iris èŠ±æœµåˆ†é¡æœå‹™"""

    def __init__(self):
        self.class_names = ["setosa", "versicolor", "virginica"]

    @bentoml.api
    def classify(self, input_data: NumpyNdarray) -> JSON:
        """
        å° Iris ç‰¹å¾µé€²è¡Œåˆ†é¡

        åƒæ•¸:
        - input_data: numpy array of shape (n_samples, 4)
                     [sepal_length, sepal_width, petal_length, petal_width]

        è¿”å›:
        - JSON: åŒ…å«é æ¸¬çµæœå’Œç½®ä¿¡åº¦
        """
        # ç¢ºä¿è¼¸å…¥å½¢ç‹€æ­£ç¢º
        if input_data.ndim == 1:
            input_data = input_data.reshape(1, -1)

        # é æ¸¬ (ä½¿ç”¨ runner æ¨¡å¼)
        predictions = iris_model_runner.predict.run(input_data)
        probabilities = iris_model_runner.predict_proba.run(input_data)

        results = []
        for i, (pred, prob) in enumerate(zip(predictions, probabilities)):
            result = {
                "prediction": self.class_names[pred],
                "prediction_id": int(pred),
                "confidence": float(max(prob)),
                "probabilities": {
                    self.class_names[j]: float(prob[j])
                    for j in range(len(self.class_names))
                }
            }
            results.append(result)

        return {"predictions": results}

    @bentoml.api
    def classify_json(self, input_data: JSON) -> JSON:
        """
        ä½¿ç”¨ JSON æ ¼å¼é€²è¡Œåˆ†é¡

        åƒæ•¸:
        - input_data: JSON æ ¼å¼çš„ç‰¹å¾µæ•¸æ“š

        è¿”å›:
        - JSON: åŒ…å«é æ¸¬çµæœå’Œç½®ä¿¡åº¦
        """
        try:
            # è§£æè¼¸å…¥æ•¸æ“š
            if "instances" in input_data:
                # æ‰¹æ¬¡é æ¸¬æ ¼å¼
                features_list = []
                for instance in input_data["instances"]:
                    features = [
                        instance["sepal_length"],
                        instance["sepal_width"],
                        instance["petal_length"],
                        instance["petal_width"]
                    ]
                    features_list.append(features)
                input_array = np.array(features_list)
            else:
                # å–®å€‹é æ¸¬æ ¼å¼
                features = [
                    input_data["sepal_length"],
                    input_data["sepal_width"],
                    input_data["petal_length"],
                    input_data["petal_width"]
                ]
                input_array = np.array([features])

            # èª¿ç”¨åˆ†é¡æ–¹æ³•
            return self.classify(input_array)

        except KeyError as e:
            return {"error": f"Missing required field: {str(e)}"}
        except Exception as e:
            return {"error": f"Prediction error: {str(e)}"}

    @bentoml.api
    def health_check(self) -> JSON:
        """å¥åº·æª¢æŸ¥ç«¯é»"""
        return {
            "status": "healthy",
            "model_name": "iris_classifier",
            "version": "1.0.0",
            "classes": self.class_names
        }
```

### æ­¥é©Ÿ 2ï¼šå»ºç«‹ BentoML é…ç½®æ–‡ä»¶

å‰µå»º `application/inference/services/bentofile.yaml`ï¼š

```yaml
service: "iris_service:IrisClassifier"
labels:
  owner: mlops-team
  project: iris-classification
  environment: production
include:
  - "iris_service.py"
exclude:
  - "tests/"
  - "*.pyc"
python:
  packages:
    - scikit-learn
    - pandas
    - numpy
    - pydantic
docker:
  distro: debian
  python_version: "3.9"
  cuda_version: null
```

### æ­¥é©Ÿ 3ï¼šæ¸¬è©¦æœå‹™

å‰µå»ºæ¸¬è©¦è…³æœ¬ `application/inference/services/test_service.py`ï¼š

```python
import requests
import json
import numpy as np

def test_numpy_endpoint():
    """æ¸¬è©¦ numpy ç«¯é»"""
    print("ğŸ§ª æ¸¬è©¦ NumPy ç«¯é»...")

    # æ¸¬è©¦æ•¸æ“š (Iris Setosa)
    test_data = np.array([[5.1, 3.5, 1.4, 0.2]])

    response = requests.post(
        "http://localhost:3000/classify",
        headers={"Content-Type": "application/json"},
        json=test_data.tolist()
    )

    if response.status_code == 200:
        result = response.json()
        print("âœ… NumPy ç«¯é»æ¸¬è©¦æˆåŠŸ")
        print(f"é æ¸¬çµæœ: {result}")
    else:
        print(f"âŒ NumPy ç«¯é»æ¸¬è©¦å¤±æ•—: {response.status_code}")
        print(response.text)

def test_json_endpoint():
    """æ¸¬è©¦ JSON ç«¯é»"""
    print("\nğŸ§ª æ¸¬è©¦ JSON ç«¯é»...")

    # å–®å€‹é æ¸¬
    test_data = {
        "sepal_length": 5.1,
        "sepal_width": 3.5,
        "petal_length": 1.4,
        "petal_width": 0.2
    }

    response = requests.post(
        "http://localhost:3000/classify_json",
        headers={"Content-Type": "application/json"},
        json=test_data
    )

    if response.status_code == 200:
        result = response.json()
        print("âœ… JSON ç«¯é»æ¸¬è©¦æˆåŠŸ")
        print(f"é æ¸¬çµæœ: {result}")
    else:
        print(f"âŒ JSON ç«¯é»æ¸¬è©¦å¤±æ•—: {response.status_code}")
        print(response.text)

def test_batch_endpoint():
    """æ¸¬è©¦æ‰¹æ¬¡é æ¸¬ç«¯é»"""
    print("\nğŸ§ª æ¸¬è©¦æ‰¹æ¬¡é æ¸¬ç«¯é»...")

    # æ‰¹æ¬¡é æ¸¬
    test_data = {
        "instances": [
            {"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2},
            {"sepal_length": 6.2, "sepal_width": 2.9, "petal_length": 4.3, "petal_width": 1.3},
            {"sepal_length": 7.3, "sepal_width": 2.9, "petal_length": 6.3, "petal_width": 1.8}
        ]
    }

    response = requests.post(
        "http://localhost:3000/classify_json",
        headers={"Content-Type": "application/json"},
        json=test_data
    )

    if response.status_code == 200:
        result = response.json()
        print("âœ… æ‰¹æ¬¡é æ¸¬æ¸¬è©¦æˆåŠŸ")
        print(f"é æ¸¬çµæœ: {json.dumps(result, indent=2, ensure_ascii=False)}")
    else:
        print(f"âŒ æ‰¹æ¬¡é æ¸¬æ¸¬è©¦å¤±æ•—: {response.status_code}")
        print(response.text)

def test_health_check():
    """æ¸¬è©¦å¥åº·æª¢æŸ¥ç«¯é»"""
    print("\nğŸ§ª æ¸¬è©¦å¥åº·æª¢æŸ¥ç«¯é»...")

    response = requests.get("http://localhost:3000/health_check")

    if response.status_code == 200:
        result = response.json()
        print("âœ… å¥åº·æª¢æŸ¥æ¸¬è©¦æˆåŠŸ")
        print(f"æœå‹™ç‹€æ…‹: {result}")
    else:
        print(f"âŒ å¥åº·æª¢æŸ¥æ¸¬è©¦å¤±æ•—: {response.status_code}")
        print(response.text)

if __name__ == "__main__":
    print("ğŸš€ é–‹å§‹æ¸¬è©¦ Iris åˆ†é¡æœå‹™...")
    print("è«‹ç¢ºä¿æœå‹™æ­£åœ¨é‹è¡Œ: poetry run bentoml serve iris_service.py:IrisClassifier --reload")
    print("-" * 60)

    test_health_check()
    test_numpy_endpoint()
    test_json_endpoint()
    test_batch_endpoint()

    print("\nâœ… æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼")
```

---

# ç¬¬å››ç« ï¼šè‡ªå‹•åŒ– CI/CD æµæ°´ç·š

## âš™ï¸ 4.1 è¨­ç½® GitHub Actions

### æ­¥é©Ÿ 1ï¼šå»ºç«‹ CI/CD å·¥ä½œæµ

å‰µå»º `infrastructure/cicd/github_actions/ml_pipeline.yml`ï¼š

```yaml
name: MLOps Pipeline

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'domain/models/**'
      - 'application/training/**'
      - 'application/inference/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'domain/models/**'
      - 'application/training/**'
      - 'application/inference/**'
  workflow_dispatch:
    inputs:
      deploy_environment:
        description: 'Deployment Environment'
        required: true
        default: 'staging'
        type: choice
        options:
        - staging
        - production

env:
  PYTHON_VERSION: "3.9"
  POETRY_VERSION: "1.6.1"

jobs:
  code-quality:
    runs-on: ubuntu-latest
    name: Code Quality Checks

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: ${{ env.POETRY_VERSION }}

    - name: Cache Poetry dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pypoetry
        key: ${{ runner.os }}-poetry-${{ hashFiles('**/poetry.lock') }}
        restore-keys: |
          ${{ runner.os }}-poetry-

    - name: Install dependencies
      run: |
        poetry config virtualenvs.create true
        poetry config virtualenvs.in-project true
        poetry install --with dev

    - name: Code formatting check
      run: |
        poetry run black --check shared/ domain/ application/ tests/

    - name: Lint code
      run: |
        poetry run pylint --disable=R,C shared/ domain/ application/

    - name: Type checking
      run: |
        poetry run mypy shared/ domain/ application/ --ignore-missing-imports

    - name: Security scan
      run: |
        poetry run bandit -r shared/ domain/ application/ -f json -o bandit-report.json || true

    - name: Upload security scan results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-scan-results
        path: bandit-report.json

  unit-tests:
    runs-on: ubuntu-latest
    name: Unit Tests
    needs: code-quality

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: ${{ env.POETRY_VERSION }}

    - name: Install dependencies
      run: |
        poetry install --with dev

    - name: Run unit tests
      run: |
        poetry run pytest tests/unit/ -v --cov=shared --cov=domain --cov=application \
          --cov-report=xml --cov-report=html --cov-report=term-missing

    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results
        path: |
          coverage.xml
          htmlcov/

  model-training:
    runs-on: ubuntu-latest
    name: Model Training & Validation
    needs: unit-tests
    if: github.event_name == 'push' || github.event.inputs.deploy_environment

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: ${{ env.POETRY_VERSION }}

    - name: Install dependencies
      run: |
        poetry install --with dev --extras all

    - name: Setup MLflow tracking
      run: |
        mkdir -p mlruns
        poetry run python -c "import mlflow; mlflow.create_experiment('ci_cd_pipeline')" || true

    - name: Run model training
      env:
        MLFLOW_TRACKING_URI: sqlite:///mlflow.db
      run: |
        poetry run python application/training/pipelines/iris_training_pipeline.py \
          --config application/training/configs/iris_config.json

    - name: Model validation
      run: |
        poetry run python application/validation/model_validation/validate_model.py \
          --model-path application/registry/model_registry/ \
          --threshold 0.95

    - name: Upload model artifacts
      uses: actions/upload-artifact@v4
      with:
        name: model-artifacts
        path: |
          application/registry/model_registry/
          mlflow.db
          mlruns/

  build-service:
    runs-on: ubuntu-latest
    name: Build BentoML Service
    needs: model-training
    if: github.event_name == 'push' || github.event.inputs.deploy_environment

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download model artifacts
      uses: actions/download-artifact@v4
      with:
        name: model-artifacts
        path: ./

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: ${{ env.POETRY_VERSION }}

    - name: Install dependencies
      run: |
        poetry install --with dev --extras all

    - name: Build BentoML service
      run: |
        cd application/inference/services
        poetry run bentoml build

    - name: Export BentoML service
      run: |
        BENTO_TAG=$(poetry run bentoml list iris_classifier --output json | jq -r '.[0].tag')
        poetry run bentoml export $BENTO_TAG ./iris_classifier_service.bento

    - name: Upload BentoML service
      uses: actions/upload-artifact@v4
      with:
        name: bento-service
        path: iris_classifier_service.bento

  integration-tests:
    runs-on: ubuntu-latest
    name: Integration Tests
    needs: build-service

    services:
      bentoml:
        image: bentoml/bentoml:latest
        ports:
          - 3000:3000

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download BentoML service
      uses: actions/download-artifact@v4
      with:
        name: bento-service
        path: ./

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install test dependencies
      run: |
        pip install requests pytest

    - name: Start BentoML service
      run: |
        bentoml import iris_classifier_service.bento
        BENTO_TAG=$(bentoml list iris_classifier --output json | jq -r '.[0].tag')
        bentoml serve $BENTO_TAG --port 3000 &
        sleep 30  # Wait for service to start

    - name: Run integration tests
      run: |
        python application/inference/services/test_service.py

    - name: Run load tests
      run: |
        python tests/integration/test_load_performance.py

  deploy-staging:
    runs-on: ubuntu-latest
    name: Deploy to Staging
    needs: integration-tests
    if: github.ref == 'refs/heads/develop' || github.event.inputs.deploy_environment == 'staging'
    environment: staging

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download BentoML service
      uses: actions/download-artifact@v4
      with:
        name: bento-service
        path: ./

    - name: Deploy to staging
      run: |
        echo "ğŸš€ éƒ¨ç½²åˆ° Staging ç’°å¢ƒ"
        # é€™è£¡æ·»åŠ æ‚¨çš„éƒ¨ç½²è…³æœ¬
        # ä¾‹å¦‚ï¼šéƒ¨ç½²åˆ° AWS ECS, GKE, æˆ–å…¶ä»–å¹³å°

    - name: Run smoke tests
      run: |
        echo "ğŸ§ª åŸ·è¡Œå†’ç…™æ¸¬è©¦"
        # æ·»åŠ éƒ¨ç½²å¾Œçš„é©—è­‰æ¸¬è©¦

  deploy-production:
    runs-on: ubuntu-latest
    name: Deploy to Production
    needs: integration-tests
    if: github.ref == 'refs/heads/main' || github.event.inputs.deploy_environment == 'production'
    environment: production

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download BentoML service
      uses: actions/download-artifact@v4
      with:
        name: bento-service
        path: ./

    - name: Deploy to production
      run: |
        echo "ğŸš€ éƒ¨ç½²åˆ° Production ç’°å¢ƒ"
        # é€™è£¡æ·»åŠ æ‚¨çš„ç”Ÿç”¢éƒ¨ç½²è…³æœ¬

    - name: Run production smoke tests
      run: |
        echo "ğŸ§ª åŸ·è¡Œç”Ÿç”¢ç’°å¢ƒå†’ç…™æ¸¬è©¦"
        # æ·»åŠ ç”Ÿç”¢ç’°å¢ƒé©—è­‰æ¸¬è©¦

    - name: Notify deployment
      run: |
        echo "ğŸ“¢ é€šçŸ¥éƒ¨ç½²å®Œæˆ"
        # æ·»åŠ é€šçŸ¥é‚è¼¯ï¼ˆSlack, Email, ç­‰ï¼‰
```

---

# ç¬¬äº”ç« ï¼šæ¨¡å‹ç›£æ§èˆ‡ç¶­é‹

## ğŸ“Š 5.1 å»ºç«‹æ¨¡å‹ç›£æ§ç³»çµ±

### æ­¥é©Ÿ 1ï¼šå‰µå»ºç›£æ§æŒ‡æ¨™æ”¶é›†

å‰µå»º `infrastructure/monitoring/metrics/model_metrics.py`ï¼š

```python
import time
import json
import logging
from datetime import datetime
from typing import Dict, List, Any
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Prometheus æŒ‡æ¨™å®šç¾©
PREDICTION_COUNTER = Counter(
    'ml_predictions_total',
    'Total number of predictions made',
    ['model_name', 'version', 'class']
)

PREDICTION_LATENCY = Histogram(
    'ml_prediction_duration_seconds',
    'Time spent processing prediction requests',
    ['model_name', 'version']
)

MODEL_ACCURACY = Gauge(
    'ml_model_accuracy',
    'Current model accuracy',
    ['model_name', 'version']
)

PREDICTION_CONFIDENCE = Histogram(
    'ml_prediction_confidence',
    'Distribution of prediction confidence scores',
    ['model_name', 'version', 'class']
)

DATA_DRIFT_SCORE = Gauge(
    'ml_data_drift_score',
    'Data drift detection score',
    ['model_name', 'feature']
)

ERROR_COUNTER = Counter(
    'ml_prediction_errors_total',
    'Total number of prediction errors',
    ['model_name', 'version', 'error_type']
)

class ModelMetricsCollector:
    """æ¨¡å‹æŒ‡æ¨™æ”¶é›†å™¨"""

    def __init__(self, model_name: str, model_version: str = "1.0.0"):
        self.model_name = model_name
        self.model_version = model_version
        self.prediction_history = []
        self.ground_truth_history = []

    def record_prediction(self,
                         features: np.ndarray,
                         prediction: str,
                         confidence: float,
                         latency: float,
                         ground_truth: str = None):
        """è¨˜éŒ„å–®æ¬¡é æ¸¬æŒ‡æ¨™"""

        # è¨˜éŒ„é æ¸¬æ¬¡æ•¸
        PREDICTION_COUNTER.labels(
            model_name=self.model_name,
            version=self.model_version,
            class=prediction
        ).inc()

        # è¨˜éŒ„å»¶é²
        PREDICTION_LATENCY.labels(
            model_name=self.model_name,
            version=self.model_version
        ).observe(latency)

        # è¨˜éŒ„ç½®ä¿¡åº¦
        PREDICTION_CONFIDENCE.labels(
            model_name=self.model_name,
            version=self.model_version,
            class=prediction
        ).observe(confidence)

        # ä¿å­˜é æ¸¬æ­·å²
        prediction_record = {
            'timestamp': datetime.now(),
            'features': features.tolist(),
            'prediction': prediction,
            'confidence': confidence,
            'latency': latency
        }

        if ground_truth:
            prediction_record['ground_truth'] = ground_truth
            self.ground_truth_history.append(ground_truth)

        self.prediction_history.append(prediction_record)

        # å¦‚æœæœ‰çœŸå¯¦æ¨™ç±¤ï¼Œè¨ˆç®—æº–ç¢ºç‡
        if ground_truth and len(self.ground_truth_history) >= 10:
            recent_predictions = [p['prediction'] for p in self.prediction_history[-10:]]
            recent_ground_truth = self.ground_truth_history[-10:]

            if len(recent_predictions) == len(recent_ground_truth):
                accuracy = accuracy_score(recent_ground_truth, recent_predictions)
                MODEL_ACCURACY.labels(
                    model_name=self.model_name,
                    version=self.model_version
                ).set(accuracy)

        logger.info(f"é æ¸¬è¨˜éŒ„: {prediction} (ç½®ä¿¡åº¦: {confidence:.3f}, å»¶é²: {latency:.3f}s)")

    def record_error(self, error_type: str, error_message: str):
        """è¨˜éŒ„é æ¸¬éŒ¯èª¤"""
        ERROR_COUNTER.labels(
            model_name=self.model_name,
            version=self.model_version,
            error_type=error_type
        ).inc()

        logger.error(f"é æ¸¬éŒ¯èª¤: {error_type} - {error_message}")

    def detect_data_drift(self,
                         current_features: np.ndarray,
                         reference_features: np.ndarray,
                         feature_names: List[str]):
        """ç°¡å–®çš„è³‡æ–™æ¼‚ç§»æª¢æ¸¬"""

        for i, feature_name in enumerate(feature_names):
            current_mean = np.mean(current_features[:, i])
            reference_mean = np.mean(reference_features[:, i])
            current_std = np.std(current_features[:, i])
            reference_std = np.std(reference_features[:, i])

            # è¨ˆç®—æ¨™æº–åŒ–å·®ç•°
            mean_diff = abs(current_mean - reference_mean) / reference_std
            std_diff = abs(current_std - reference_std) / reference_std

            drift_score = max(mean_diff, std_diff)

            DATA_DRIFT_SCORE.labels(
                model_name=self.model_name,
                feature=feature_name
            ).set(drift_score)

            if drift_score > 2.0:  # è­¦å‘Šé–¾å€¼
                logger.warning(f"è³‡æ–™æ¼‚ç§»è­¦å‘Š: {feature_name} (åˆ†æ•¸: {drift_score:.3f})")

    def get_prediction_summary(self, hours: int = 24) -> Dict[str, Any]:
        """ç²å–é æ¸¬æ‘˜è¦å ±å‘Š"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_predictions = [
            p for p in self.prediction_history
            if p['timestamp'] > cutoff_time
        ]

        if not recent_predictions:
            return {"message": "No recent predictions"}

        # è¨ˆç®—çµ±è¨ˆè³‡è¨Š
        total_predictions = len(recent_predictions)
        avg_confidence = np.mean([p['confidence'] for p in recent_predictions])
        avg_latency = np.mean([p['latency'] for p in recent_predictions])

        class_distribution = {}
        for p in recent_predictions:
            pred_class = p['prediction']
            class_distribution[pred_class] = class_distribution.get(pred_class, 0) + 1

        summary = {
            "time_period_hours": hours,
            "total_predictions": total_predictions,
            "average_confidence": float(avg_confidence),
            "average_latency_seconds": float(avg_latency),
            "class_distribution": class_distribution,
            "predictions_per_hour": total_predictions / hours
        }

        # å¦‚æœæœ‰çœŸå¯¦æ¨™ç±¤ï¼Œè¨ˆç®—æº–ç¢ºç‡
        recent_with_truth = [p for p in recent_predictions if 'ground_truth' in p]
        if recent_with_truth:
            predictions = [p['prediction'] for p in recent_with_truth]
            ground_truths = [p['ground_truth'] for p in recent_with_truth]
            accuracy = accuracy_score(ground_truths, predictions)
            precision, recall, f1, _ = precision_recall_fscore_support(
                ground_truths, predictions, average='weighted'
            )

            summary.update({
                "accuracy": float(accuracy),
                "precision": float(precision),
                "recall": float(recall),
                "f1_score": float(f1),
                "labeled_predictions": len(recent_with_truth)
            })

        return summary

    def export_metrics(self, filepath: str):
        """å°å‡ºæŒ‡æ¨™åˆ°æ–‡ä»¶"""
        summary = self.get_prediction_summary()

        with open(filepath, 'w') as f:
            json.dump({
                "model_name": self.model_name,
                "model_version": self.model_version,
                "export_timestamp": datetime.now().isoformat(),
                "metrics_summary": summary,
                "recent_predictions": self.prediction_history[-100:]  # æœ€è¿‘100æ¬¡é æ¸¬
            }, f, indent=2, default=str)

        logger.info(f"æŒ‡æ¨™å·²å°å‡ºåˆ°: {filepath}")

# å•Ÿå‹• Prometheus æŒ‡æ¨™æœå‹™å™¨
def start_metrics_server(port: int = 8000):
    """å•Ÿå‹• Prometheus æŒ‡æ¨™ä¼ºæœå™¨"""
    start_http_server(port)
    logger.info(f"Prometheus æŒ‡æ¨™ä¼ºæœå™¨å·²å•Ÿå‹•ï¼Œç«¯å£: {port}")
    logger.info(f"æŒ‡æ¨™ç«¯é»: http://localhost:{port}/metrics")

if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    import time
    from datetime import timedelta

    # å•Ÿå‹•æŒ‡æ¨™ä¼ºæœå™¨
    start_metrics_server(8000)

    # å‰µå»ºæŒ‡æ¨™æ”¶é›†å™¨
    collector = ModelMetricsCollector("iris_classifier", "1.0.0")

    # æ¨¡æ“¬ä¸€äº›é æ¸¬
    for i in range(50):
        features = np.random.rand(1, 4)
        prediction = np.random.choice(['setosa', 'versicolor', 'virginica'])
        confidence = np.random.uniform(0.7, 0.99)
        latency = np.random.uniform(0.001, 0.050)

        collector.record_prediction(features, prediction, confidence, latency)
        time.sleep(0.1)

    # ç”Ÿæˆå ±å‘Š
    summary = collector.get_prediction_summary(1)  # æœ€è¿‘1å°æ™‚
    print("é æ¸¬æ‘˜è¦:", json.dumps(summary, indent=2))

    # å°å‡ºæŒ‡æ¨™
    collector.export_metrics("model_metrics_export.json")

    print("æŒ‡æ¨™æ”¶é›†å®Œæˆï¼è¨ªå• http://localhost:8000/metrics æŸ¥çœ‹ Prometheus æŒ‡æ¨™")
```

### æ­¥é©Ÿ 2ï¼šæ•´åˆç›£æ§åˆ° BentoML æœå‹™

æ›´æ–° `application/inference/services/iris_service.py`ï¼š

```python
import numpy as np
import pandas as pd
import time
from typing import List
import bentoml
from bentoml.io import NumpyNdarray, JSON
from pydantic import BaseModel

# å°å…¥ç›£æ§çµ„ä»¶
import sys
from pathlib import Path
project_root = Path(__file__).parent.parent.parent.parent
sys.path.append(str(project_root))

from infrastructure.monitoring.metrics.model_metrics import ModelMetricsCollector, start_metrics_server

# å®šç¾©è¼¸å…¥æ•¸æ“šæ¨¡å‹
class IrisFeatures(BaseModel):
    sepal_length: float
    sepal_width: float
    petal_length: float
    petal_width: float

class IrisBatch(BaseModel):
    instances: List[IrisFeatures]

# è¼‰å…¥å·²è¨“ç·´çš„æ¨¡å‹
iris_model_ref = bentoml.sklearn.get("iris_classifier_pipeline:latest")

@bentoml.service(
    resources={"cpu": "2"},
    traffic={"timeout": 20},
)
class IrisClassifier:
    """Iris èŠ±æœµåˆ†é¡æœå‹™ - å«ç›£æ§åŠŸèƒ½"""

    def __init__(self):
        self.model = iris_model_ref.load_model()
        self.class_names = ["setosa", "versicolor", "virginica"]

        # åˆå§‹åŒ–ç›£æ§
        self.metrics_collector = ModelMetricsCollector("iris_classifier", "1.0.0")

        # å•Ÿå‹• Prometheus æŒ‡æ¨™ä¼ºæœå™¨
        try:
            start_metrics_server(8001)  # ä½¿ç”¨ä¸åŒç«¯å£é¿å…è¡çª
        except Exception as e:
            print(f"æŒ‡æ¨™ä¼ºæœå™¨å•Ÿå‹•å¤±æ•—: {e}")

    @bentoml.api
    def classify(self, input_data: NumpyNdarray) -> JSON:
        """å° Iris ç‰¹å¾µé€²è¡Œåˆ†é¡ - å«ç›£æ§"""
        start_time = time.time()

        try:
            # ç¢ºä¿è¼¸å…¥å½¢ç‹€æ­£ç¢º
            if input_data.ndim == 1:
                input_data = input_data.reshape(1, -1)

            # é æ¸¬
            predictions = self.model.predict(input_data)
            probabilities = self.model.predict_proba(input_data)

            # è¨ˆç®—è™•ç†æ™‚é–“
            processing_time = time.time() - start_time

            results = []
            for i, (pred, prob) in enumerate(zip(predictions, probabilities)):
                predicted_class = self.class_names[pred]
                confidence = float(max(prob))

                result = {
                    "prediction": predicted_class,
                    "prediction_id": int(pred),
                    "confidence": confidence,
                    "probabilities": {
                        self.class_names[j]: float(prob[j])
                        for j in range(len(self.class_names))
                    },
                    "processing_time_ms": processing_time * 1000
                }
                results.append(result)

                # è¨˜éŒ„ç›£æ§æŒ‡æ¨™
                self.metrics_collector.record_prediction(
                    features=input_data[i:i+1],
                    prediction=predicted_class,
                    confidence=confidence,
                    latency=processing_time
                )

            return {"predictions": results, "total_processing_time_ms": processing_time * 1000}

        except Exception as e:
            # è¨˜éŒ„éŒ¯èª¤
            self.metrics_collector.record_error("prediction_error", str(e))
            return {"error": f"Prediction error: {str(e)}"}

    @bentoml.api
    def classify_json(self, input_data: JSON) -> JSON:
        """ä½¿ç”¨ JSON æ ¼å¼é€²è¡Œåˆ†é¡ - å«ç›£æ§"""
        start_time = time.time()

        try:
            # è§£æè¼¸å…¥æ•¸æ“š
            if "instances" in input_data:
                # æ‰¹æ¬¡é æ¸¬æ ¼å¼
                features_list = []
                for instance in input_data["instances"]:
                    features = [
                        instance["sepal_length"],
                        instance["sepal_width"],
                        instance["petal_length"],
                        instance["petal_width"]
                    ]
                    features_list.append(features)
                input_array = np.array(features_list)
            else:
                # å–®å€‹é æ¸¬æ ¼å¼
                features = [
                    input_data["sepal_length"],
                    input_data["sepal_width"],
                    input_data["petal_length"],
                    input_data["petal_width"]
                ]
                input_array = np.array([features])

            # èª¿ç”¨åˆ†é¡æ–¹æ³•
            return self.classify(input_array)

        except KeyError as e:
            error_msg = f"Missing required field: {str(e)}"
            self.metrics_collector.record_error("input_error", error_msg)
            return {"error": error_msg}
        except Exception as e:
            error_msg = f"Prediction error: {str(e)}"
            self.metrics_collector.record_error("general_error", error_msg)
            return {"error": error_msg}

    @bentoml.api
    def health_check(self) -> JSON:
        """å¥åº·æª¢æŸ¥ç«¯é»"""
        return {
            "status": "healthy",
            "model_name": "iris_classifier",
            "version": "1.0.0",
            "classes": self.class_names,
            "metrics_endpoint": "http://localhost:8001/metrics"
        }

    @bentoml.api
    def get_metrics_summary(self, hours: int = 1) -> JSON:
        """ç²å–æ¨¡å‹æŒ‡æ¨™æ‘˜è¦"""
        try:
            summary = self.metrics_collector.get_prediction_summary(hours)
            return {"status": "success", "metrics": summary}
        except Exception as e:
            return {"status": "error", "message": str(e)}

    @bentoml.api
    def feedback(self, input_data: JSON) -> JSON:
        """æ¥æ”¶é æ¸¬åé¥‹ç”¨æ–¼ç›£æ§"""
        try:
            # è§£æåé¥‹æ•¸æ“š
            features = np.array([[
                input_data["sepal_length"],
                input_data["sepal_width"],
                input_data["petal_length"],
                input_data["petal_width"]
            ]])

            predicted_class = input_data["predicted_class"]
            actual_class = input_data["actual_class"]
            confidence = input_data.get("confidence", 0.0)

            # è¨˜éŒ„å¸¶æœ‰çœŸå¯¦æ¨™ç±¤çš„é æ¸¬
            self.metrics_collector.record_prediction(
                features=features,
                prediction=predicted_class,
                confidence=confidence,
                latency=0.0,  # åé¥‹æ™‚ä¸è¨˜éŒ„å»¶é²
                ground_truth=actual_class
            )

            return {"status": "feedback_recorded", "message": "æ„Ÿè¬æ‚¨çš„åé¥‹ï¼"}

        except KeyError as e:
            return {"status": "error", "message": f"Missing field: {str(e)}"}
        except Exception as e:
            return {"status": "error", "message": str(e)}
```

---

# ç¬¬å…­ç« ï¼šBentoML æœ€ä½³å¯¦è¸èˆ‡å¸¸è¦‹å•é¡Œ

## ğŸš€ 6.1 BentoML ç‰ˆæœ¬å…¼å®¹æ€§

### é‡è¦ï¼šAPI è®ŠåŒ–èªªæ˜

BentoML åœ¨ç‰ˆæœ¬æ›´æ–°ä¸­æœ‰é‡å¤§ API è®ŠåŒ–ï¼Œä»¥ä¸‹æ˜¯é—œéµå·®ç•°ï¼š

#### èˆŠç‰ˆæœ¬ (ä¸æ¨è–¦)
```python
import bentoml
from bentoml.io import NumpyNdarray

# èˆŠçš„æœå‹™å®šç¾©æ–¹å¼
@bentoml.service(
    resources={"cpu": "2"},
    traffic={"timeout": 20},
)
class MyService:
    def __init__(self):
        self.model = bentoml.sklearn.get("model:latest").load_model()
    
    @bentoml.api
    def predict(self, input_data: NumpyNdarray):
        return self.model.predict(input_data)
```

#### æ–°ç‰ˆæœ¬ (æ¨è–¦)
```python
import bentoml
from bentoml.io import NumpyNdarray

# æ–°çš„æœå‹™å®šç¾©æ–¹å¼
iris_model_runner = bentoml.sklearn.get("iris_clf:latest").to_runner()

@bentoml.Service(
    name="iris_classifier",
    runners=[iris_model_runner],
)
class MyService:
    def __init__(self):
        self.class_names = ["setosa", "versicolor", "virginica"]
    
    @bentoml.api
    def predict(self, input_data: NumpyNdarray):
        predictions = iris_model_runner.predict.run(input_data)
        probabilities = iris_model_runner.predict_proba.run(input_data)
        return {"predictions": predictions, "probabilities": probabilities}
```

### 6.2 æœå‹™å•Ÿå‹•æœ€ä½³å¯¦è¸

#### æ­£ç¢ºçš„å•Ÿå‹•å‘½ä»¤
```bash
# æ–¹å¼ 1: æŒ‡å®šæª”æ¡ˆå’Œæœå‹™å¯¦ä¾‹ (æ¨è–¦)
poetry run bentoml serve iris_service.py:svc --reload

# æ–¹å¼ 2: ä½¿ç”¨ bentofile.yaml
poetry run bentoml serve . --reload

# æ–¹å¼ 3: æŠ‘åˆ¶è­¦å‘Š (ç”Ÿç”¢ç’°å¢ƒ)
PYTHONWARNINGS="ignore" poetry run bentoml serve iris_service.py:svc --reload
```

#### å¸¸è¦‹å•Ÿå‹•éŒ¯èª¤èˆ‡è§£æ±ºæ–¹æ¡ˆ

| éŒ¯èª¤è¨Šæ¯ | åŸå›  | è§£æ±ºæ–¹æ¡ˆ |
|---------|------|---------|
| `AttributeError: module 'bentoml' has no attribute 'service'` | ä½¿ç”¨äº†èˆŠçš„ API | æ”¹ç”¨ `@bentoml.Service` |
| `TypeError: Service.__init__() got an unexpected keyword argument 'resources'` | åƒæ•¸ä½ç½®éŒ¯èª¤ | å°‡ `resources` ç§»åˆ° `@bentoml.api` |
| `Attribute "IrisClassifier" not found in module` | æœå‹™å¯¦ä¾‹åç¨±éŒ¯èª¤ | æª¢æŸ¥æœå‹™å¯¦ä¾‹åç¨±ï¼Œé€šå¸¸æ˜¯ `svc` |
| `UserWarning: pkg_resources is deprecated` | ç¬¬ä¸‰æ–¹ä¾è³´è­¦å‘Š | ä½¿ç”¨ `PYTHONWARNINGS="ignore"` |

### 6.3 æ¨¡å‹ç®¡ç†æœ€ä½³å¯¦è¸

#### æ¨¡å‹è¨»å†Šåˆ° BentoML
```python
import bentoml
from sklearn.ensemble import RandomForestClassifier

# è¨“ç·´æ¨¡å‹
model = RandomForestClassifier()
model.fit(X_train, y_train)

# ä¿å­˜åˆ° BentoML æ¨¡å‹åº«
bento_model = bentoml.sklearn.save_model(
    "iris_clf",  # æ¨¡å‹åç¨±
    model,       # æ¨¡å‹ç‰©ä»¶
    signatures={
        "predict": {"batchable": True, "batch_dim": 0},
    },
    labels={
        "owner": "mlops-team",
        "stage": "dev",
        "accuracy": f"{accuracy:.4f}"
    }
)
print(f"æ¨¡å‹å·²ä¿å­˜: {bento_model.tag}")
```

#### æ¨¡å‹è¼‰å…¥å’Œä½¿ç”¨
```python
# è¼‰å…¥æ¨¡å‹ä¸¦å‰µå»º runner
model_runner = bentoml.sklearn.get("iris_clf:latest").to_runner()

# åœ¨æœå‹™ä¸­ä½¿ç”¨
@bentoml.Service(
    name="iris_service",
    runners=[model_runner],
)
class IrisService:
    @bentoml.api
    def predict(self, input_data):
        return model_runner.predict.run(input_data)
```

### 6.4 é™¤éŒ¯æŠ€å·§

#### æª¢æŸ¥ BentoML æ¨¡å‹åº«
```bash
# åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
poetry run bentoml models list

# æŸ¥çœ‹ç‰¹å®šæ¨¡å‹è©³æƒ…
poetry run bentoml models get iris_clf:latest

# æª¢æŸ¥æœå‹™ç‹€æ…‹
poetry run bentoml services list
```

#### æ¸¬è©¦æœå‹™
```bash
# å¥åº·æª¢æŸ¥
curl http://localhost:3000/health_check

# æ¸¬è©¦é æ¸¬
curl -X POST http://localhost:3000/classify \
  -H "Content-Type: application/json" \
  -d '[[5.1, 3.5, 1.4, 0.2]]'
```

---

# ç¬¬ä¸ƒç« ï¼šMakefile å‘½ä»¤åƒè€ƒæŒ‡å—

## ğŸš€ 7.1 Makefile å‘½ä»¤ç¸½è¦½

æœ¬å°ˆæ¡ˆæä¾›äº†å®Œæ•´çš„ Makefile ä¾†ç°¡åŒ–é–‹ç™¼æµç¨‹ã€‚ä»¥ä¸‹æ˜¯æ‰€æœ‰å¯ç”¨å‘½ä»¤çš„è©³ç´°èªªæ˜ï¼š

### ç’°å¢ƒè¨­ç½®å‘½ä»¤

#### `make install` - å®Œæ•´ä¾è³´å®‰è£ (æ¨è–¦)
```bash
make install
```
**åŠŸèƒ½**:
- å®‰è£ Poetry æ‰€æœ‰ä¾è³´é …
- é…ç½® PyTorch CUDA æ”¯æŒ
- å®‰è£ TensorFlow GPU æ”¯æŒ
- å®‰è£ OpenAI Whisper
**é©ç”¨å ´æ™¯**: å®Œæ•´é–‹ç™¼ç’°å¢ƒè¨­ç½®

#### `make install-dev` - æœ€å°åŒ–é–‹ç™¼å·¥å…·å®‰è£
```bash
make install-dev
```
**åŠŸèƒ½**:
- å®‰è£åŸºæœ¬é–‹ç™¼å·¥å…· (black, pylint, pytest, jupyter)
- è·³éå¤§å‹ ML ä¾è³´é …
**é©ç”¨å ´æ™¯**: å¿«é€Ÿè¨­ç½®æˆ– CI ç’°å¢ƒ

### é–‹ç™¼å·¥ä½œæµå‘½ä»¤

#### `make refactor` - ä»£ç¢¼é‡æ§‹ (æ¨è–¦)
```bash
make refactor
```
**åŠŸèƒ½**: åŒæ™‚é‹è¡Œæ ¼å¼åŒ–å’Œä»£ç¢¼æª¢æŸ¥
- ç­‰åŒæ–¼: `make format && make lint`
**é©ç”¨å ´æ™¯**: ä»£ç¢¼æäº¤å‰çš„å“è³ªæª¢æŸ¥

#### `make format` - ä»£ç¢¼æ ¼å¼åŒ–
```bash
make format
```
**åŠŸèƒ½**: ä½¿ç”¨ Black çµ±ä¸€ä»£ç¢¼æ ¼å¼
**é©ç”¨å ´æ™¯**: çµ±ä¸€ä»£ç¢¼é¢¨æ ¼

#### `make lint` - ä»£ç¢¼å“è³ªæª¢æŸ¥
```bash
make lint
```
**åŠŸèƒ½**: ä½¿ç”¨ Pylint æª¢æŸ¥ä»£ç¢¼å“è³ª
**é©ç”¨å ´æ™¯**: è­˜åˆ¥ä»£ç¢¼å•é¡Œå’Œæ”¹é€²é»

#### `make test` - é‹è¡Œæ¸¬è©¦å¥—ä»¶
```bash
make test
```
**åŠŸèƒ½**:
- é‹è¡Œæ‰€æœ‰ pytest æ¸¬è©¦
- ç”Ÿæˆè¦†è“‹ç‡å ±å‘Š
- æ¸¬è©¦ç¯„åœ: shared/, domain/, application/
**é©ç”¨å ´æ™¯**: é©—è­‰ä»£ç¢¼åŠŸèƒ½æ­£ç¢ºæ€§

#### `make clean` - æ¸…ç†æ§‹å»ºæ–‡ä»¶
```bash
make clean
```
**åŠŸèƒ½**:
- åˆªé™¤ `__pycache__` ç›®éŒ„
- æ¸…ç† `.pyc` æ–‡ä»¶
- ç§»é™¤ `dist/`, `build/`, `.coverage` æ–‡ä»¶
**é©ç”¨å ´æ™¯**: æ¸…ç†é–‹ç™¼ç’°å¢ƒæˆ–æº–å‚™ç™¼ä½ˆ

### ML èˆ‡ GPU å‘½ä»¤

#### `make checkgpu` - GPU ç’°å¢ƒé©—è­‰
```bash
make checkgpu
```
**åŠŸèƒ½**:
- é©—è­‰ PyTorch CUDA æ”¯æŒ
- æª¢æŸ¥ TensorFlow GPU æ”¯æŒ
- é¡¯ç¤º GPU è©³ç´°è³‡è¨Š
**é©ç”¨å ´æ™¯**: GPU é…ç½®é©—è­‰å’Œæ•…éšœæ’é™¤

#### `make train` - æ¨¡å‹è¨“ç·´
```bash
make train
```
**åŠŸèƒ½**: é‹è¡Œæ¨¡å‹è¨“ç·´æµæ°´ç·š
**é©ç”¨å ´æ™¯**: æ¨¡å‹è¨“ç·´å’Œå¯¦é©—

### éƒ¨ç½²èˆ‡æœå‹™å‘½ä»¤

#### `make bento-build` - æ§‹å»º BentoML æœå‹™
```bash
make bento-build
```
**åŠŸèƒ½**:
- æ ¹æ“š `bentofile.yaml` æ§‹å»ºå®Œæ•´çš„ BentoML æœå‹™åŒ…
- æ‰“åŒ…æ¨¡å‹ã€ä»£ç¢¼ã€ä¾è³´å’Œç’°å¢ƒé…ç½®
- ç”Ÿæˆç”Ÿç”¢å°±ç·’çš„æœå‹™ artifacts
**é©ç”¨å ´æ™¯**: ç”Ÿç”¢ç’°å¢ƒæº–å‚™å’Œéƒ¨ç½²

**ğŸ” è©³ç´°å·¥ä½œæµç¨‹**:

1. **è®€å–é…ç½®**: è§£æ `bentofile.yaml` ä¸­çš„æœå‹™å®šç¾©
   ```yaml
   service: "iris_service:IrisClassifier"
   include:
     - "iris_service.py"
   python:
     packages:
       - scikit-learn
       - numpy
   ```

2. **æ¨¡å‹æ‰“åŒ…**: å¾ BentoML store è¼‰å…¥å·²è¨“ç·´çš„æ¨¡å‹
   ```python
   # å¾è¨“ç·´è…³æœ¬ä¿å­˜çš„æ¨¡å‹
   iris_model_runner = bentoml.sklearn.get("iris_clf:latest").to_runner()
   ```

3. **ä¾è³´åˆ†æ**: è‡ªå‹•æª¢æ¸¬å’Œæ‰“åŒ…æ‰€æœ‰å¿…è¦çš„ Python åŒ…

4. **ç’°å¢ƒå°è£**: å‰µå»ºéš”é›¢çš„ Python ç’°å¢ƒ

5. **æœå‹™æ§‹å»º**: ç”ŸæˆåŒ…å«æ‰€æœ‰çµ„ä»¶çš„å¯åŸ·è¡Œæœå‹™åŒ…

**ğŸ“¦ è¼¸å‡ºçµæœ**:
- BentoML æœå‹™åŒ… (åŒ…å«æ¨¡å‹äºŒé€²åˆ¶æ–‡ä»¶)
- æœå‹™å…ƒæ•¸æ“šå’Œé…ç½®
- Python ç’°å¢ƒå¿«ç…§
- Docker æ§‹å»ºé…ç½® (å¦‚æœéœ€è¦)

**âš ï¸ å‰ç½®æ¢ä»¶**:
- å¿…é ˆå…ˆé‹è¡Œæ¨¡å‹è¨“ç·´è…³æœ¬ä¿å­˜æ¨¡å‹åˆ° BentoML store
- éœ€è¦æ­£ç¢ºçš„ `bentofile.yaml` é…ç½®
- æ‰€æœ‰ä¾è³´å¿…é ˆå¯ç”¨

#### `make containerize` - å®¹å™¨åŒ–æœå‹™
```bash
make containerize
```
**åŠŸèƒ½**: å‰µå»º Docker å®¹å™¨æ˜ åƒ
**é©ç”¨å ´æ™¯**: å®¹å™¨åŒ–éƒ¨ç½²æº–å‚™

#### `make run` - å•Ÿå‹•æœ¬åœ°æœå‹™ (ç„¡è­¦å‘Š)
```bash
make run
```
**åŠŸèƒ½**:
- å•Ÿå‹• BentoML æœå‹™
- å·²åŒ…å« `PYTHONWARNINGS="ignore"` ä¾†æŠ‘åˆ¶è­¦å‘Š
- æ”¯æŒ `--reload` ç†±é‡è¼‰
**é©ç”¨å ´æ™¯**: æœ¬åœ°é–‹ç™¼å’Œæ¸¬è©¦

#### `make deploy` - éƒ¨ç½²æœå‹™
```bash
make deploy
```
**åŠŸèƒ½**: éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ (ç›®å‰ç‚ºä½”ä½ç¬¦)
**é©ç”¨å ´æ™¯**: ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²

### ç¶œåˆå‘½ä»¤

#### `make all` - å®Œæ•´é–‹ç™¼æµæ°´ç·š
```bash
make all
```
**åŸ·è¡Œé †åº**: `install` â†’ `format` â†’ `lint` â†’ `test` â†’ `checkgpu`
**é©ç”¨å ´æ™¯**: å®Œæ•´ç’°å¢ƒè¨­ç½®å’Œé©—è­‰

#### `make help` - é¡¯ç¤ºå¹«åŠ© (é»˜èªå‘½ä»¤)
```bash
make help  # æˆ–åªè¼¸å…¥: make
```
**åŠŸèƒ½**: é¡¯ç¤ºæ‰€æœ‰å¯ç”¨å‘½ä»¤èªªæ˜
**é©ç”¨å ´æ™¯**: æŸ¥çœ‹å‘½ä»¤å¹«åŠ©

## 7.2 å¸¸è¦‹ä½¿ç”¨æ¨¡å¼

### åˆæ¬¡è¨­ç½®
```bash
# å®Œæ•´ç’°å¢ƒè¨­ç½®
make install && make checkgpu

# é©—è­‰è¨­ç½®
make all
```

### æ—¥å¸¸é–‹ç™¼
```bash
# ä»£ç¢¼æ”¹é€²
make refactor

# åŠŸèƒ½æ¸¬è©¦
make test

# æ¸…ç†ç’°å¢ƒ
make clean
```

### ç”Ÿç”¢éƒ¨ç½²
```bash
# æ§‹å»ºæœå‹™
make bento-build

# å®¹å™¨åŒ–
make containerize

# æœ¬åœ°æ¸¬è©¦
make run
```

### CI/CD æµæ°´ç·š
```bash
# è‡ªå‹•åŒ–æª¢æŸ¥
make format && make lint && make test
```

## 7.3 æ•…éšœæ’é™¤

### å‘½ä»¤åŸ·è¡Œå¤±æ•—
```bash
# æŸ¥çœ‹è©³ç´°å¹«åŠ©
make help

# æª¢æŸ¥ Poetry ç’°å¢ƒ
poetry env info

# é‡æ–°å®‰è£ä¾è³´
make clean && make install
```

### GPU ç›¸é—œå•é¡Œ
```bash
# æª¢æŸ¥ GPU æ”¯æŒ
make checkgpu

# é©—è­‰ CUDA å®‰è£
nvidia-smi
```

### æœå‹™å•Ÿå‹•å•é¡Œ
```bash
# æª¢æŸ¥æœå‹™ç‹€æ…‹
poetry run bentoml list

# æŸ¥çœ‹æœå‹™æ—¥èªŒ
tail -f bentoml_service.log
```

---

## ğŸ“ ç¸½çµèˆ‡æœ€ä½³å¯¦è¸

### å®Œæˆçš„ MLOps ç³»çµ±åŒ…å«ï¼š

1. **ğŸ—ï¸ ç³»çµ±æ¶æ§‹** - Domain/Application/Infrastructure åˆ†å±¤
2. **ğŸ“¦ ä¾è³´ç®¡ç†** - Poetry çµ±ä¸€ç®¡ç†æ‰€æœ‰ä¾è³´
3. **ğŸ”¬ å¯¦é©—ç®¡ç†** - Jupyter + MLflow å¯¦é©—è¿½è¹¤
4. **ğŸš€ è‡ªå‹•åŒ–è¨“ç·´** - å¯é‡è¤‡çš„è¨“ç·´æµæ°´ç·š
5. **ğŸŒ æœå‹™éƒ¨ç½²** - BentoML é«˜æ€§èƒ½æ¨è«–æœå‹™
6. **âš™ï¸ CI/CD æµæ°´ç·š** - GitHub Actions è‡ªå‹•åŒ–éƒ¨ç½²
7. **ğŸ“Š ç›£æ§ç³»çµ±** - Prometheus + è‡ªå®šç¾©æŒ‡æ¨™æ”¶é›†
8. **ğŸ§ª æ¸¬è©¦è¦†è“‹** - å–®å…ƒæ¸¬è©¦ã€æ•´åˆæ¸¬è©¦ã€è² è¼‰æ¸¬è©¦

### MLOps æœ€ä½³å¯¦è¸ï¼š

âœ… **ç‰ˆæœ¬æ§åˆ¶ä¸€åˆ‡** - ä»£ç¢¼ã€æ•¸æ“šã€æ¨¡å‹ã€é…ç½®
âœ… **è‡ªå‹•åŒ–æµæ°´ç·š** - å¾è¨“ç·´åˆ°éƒ¨ç½²å…¨è‡ªå‹•åŒ–
âœ… **æŒçºŒç›£æ§** - æ¨¡å‹æ•ˆèƒ½ã€è³‡æ–™æ¼‚ç§»ã€ç³»çµ±å¥åº·
âœ… **å¯é‡ç¾æ€§** - ç¢ºå®šæ€§æ§‹å»ºå’Œå¯é‡è¤‡å¯¦é©—
âœ… **å¿«é€Ÿåé¥‹** - å¿«é€Ÿå¯¦é©—è¿­ä»£å’Œéƒ¨ç½²å›æ»¾
âœ… **å®‰å…¨ç¬¬ä¸€** - æ†‘è­‰ç®¡ç†å’Œå­˜å–æ§åˆ¶

**æ­å–œï¼æ‚¨ç¾åœ¨æ“æœ‰ä¸€å€‹å®Œæ•´çš„ç”Ÿç”¢ç´š MLOps ç³»çµ±ï¼** ğŸ‰

---

ä¸‹ä¸€æ­¥æ‚¨å¯ä»¥ï¼š
1. æ“´å±•åˆ°æ›´è¤‡é›œçš„æ¨¡å‹ï¼ˆæ·±åº¦å­¸ç¿’ã€NLPã€é›»è…¦è¦–è¦ºï¼‰
2. æ•´åˆé›²ç«¯å¹³å°ï¼ˆAWS SageMakerã€GCP Vertex AIï¼‰
3. å¯¦æ–½æ›´é«˜ç´šçš„ç›£æ§ï¼ˆè³‡æ–™æ¼‚ç§»æª¢æ¸¬ã€A/B æ¸¬è©¦ï¼‰
4. æ·»åŠ æ›´å¤šè‡ªå‹•åŒ–ï¼ˆè¶…åƒæ•¸èª¿å„ªã€AutoMLï¼‰