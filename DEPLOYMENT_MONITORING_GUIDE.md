# ğŸš€ éƒ¨ç½²èˆ‡ç›£æ§å®Œæ•´æŒ‡å—

## ğŸ“‹ ç›®éŒ„
- [å¿«é€Ÿé–‹å§‹](#å¿«é€Ÿé–‹å§‹)
- [æœ¬åœ°éƒ¨ç½²](#æœ¬åœ°éƒ¨ç½²)
- [å®¹å™¨åŒ–éƒ¨ç½²](#å®¹å™¨åŒ–éƒ¨ç½²)
- [é›²ç«¯éƒ¨ç½²](#é›²ç«¯éƒ¨ç½²)
- [ç›£æ§è¨­ç½®](#ç›£æ§è¨­ç½®)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1. ä¸€éµåŸ·è¡Œå®Œæ•´æµæ°´ç·š

```bash
# ä¹¾è·‘æ¨¡å¼ï¼ˆæ¸¬è©¦æµç¨‹ï¼‰
poetry run python scripts/automation/full_mlops_pipeline.py --dry-run

# å¯¦éš›åŸ·è¡Œ
poetry run python scripts/automation/full_mlops_pipeline.py --config scripts/automation/pipeline_config.json
```

### 2. åˆ†æ­¥é©ŸåŸ·è¡Œ

```bash
# æ­¥é©Ÿ 1: è¨“ç·´æ¨¡å‹
poetry run python application/training/pipelines/iris_training_pipeline.py --config application/training/configs/iris_config.json

# æ­¥é©Ÿ 2: é©—è­‰æ¨¡å‹
poetry run python application/validation/model_validation/validate_model.py --model-path application/registry/model_registry/ --threshold 0.95

# æ­¥é©Ÿ 3: å•Ÿå‹•æœå‹™
make run
```

---

## ğŸ  æœ¬åœ°éƒ¨ç½²

### é–‹ç™¼ç’°å¢ƒéƒ¨ç½²

```bash
# 1. å®‰è£ä¾è³´
make install

# 2. è¨“ç·´æ¨¡å‹
poetry run python application/training/pipelines/iris_training_pipeline.py

# 3. å•Ÿå‹• BentoML æœå‹™ (æ¨è–¦ä½¿ç”¨ Makefile)
make run

# æˆ–æ‰‹å‹•å•Ÿå‹• (ç„¡è­¦å‘Šç‰ˆæœ¬)
cd application/inference/services
PYTHONWARNINGS="ignore" poetry run bentoml serve iris_service.py:svc --reload --port 3000
```

### ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²

```bash
# 1. å»ºç«‹ BentoML æœå‹™
cd application/inference/services
poetry run bentoml build

# 2. å•Ÿå‹•æœå‹™
BENTO_TAG=$(poetry run bentoml list iris_classifier --output json | jq -r '.[0].tag')
poetry run bentoml serve $BENTO_TAG --port 3000
```

### æœå‹™æ¸¬è©¦

```bash
# åŠŸèƒ½æ¸¬è©¦
poetry run python application/inference/services/test_service.py

# è² è¼‰æ¸¬è©¦
poetry run python tests/integration/test_load_performance.py --concurrent 10 --requests 20

# å¥åº·æª¢æŸ¥
curl http://localhost:3000/health_check
```

---

## ğŸ³ å®¹å™¨åŒ–éƒ¨ç½²

### Docker åŸºç¤éƒ¨ç½²

#### 1. å»ºç«‹ Docker æ˜ åƒ

```bash
# ä½¿ç”¨ BentoML å»ºç«‹
cd application/inference/services
poetry run bentoml build
BENTO_TAG=$(poetry run bentoml list iris_classifier --output json | jq -r '.[0].tag')
poetry run bentoml containerize $BENTO_TAG --platform linux/amd64
```

#### 2. é‹è¡Œå®¹å™¨

```bash
# ç²å–æ˜ åƒåç¨±
DOCKER_IMAGE=$(docker images --format "table {{.Repository}}:{{.Tag}}" | grep iris_classifier | head -1)

# é‹è¡Œå®¹å™¨
docker run -p 3000:3000 $DOCKER_IMAGE

# èƒŒæ™¯é‹è¡Œ
docker run -d -p 3000:3000 --name iris-service $DOCKER_IMAGE
```

#### 3. å®¹å™¨ç®¡ç†

```bash
# æŸ¥çœ‹æ—¥èªŒ
docker logs iris-service

# é€²å…¥å®¹å™¨
docker exec -it iris-service /bin/bash

# åœæ­¢å®¹å™¨
docker stop iris-service

# ç§»é™¤å®¹å™¨
docker rm iris-service
```

### Docker Compose éƒ¨ç½²

å‰µå»º `docker-compose.yml`ï¼š

```yaml
version: '3.8'

services:
  iris-service:
    build:
      context: .
      dockerfile: infrastructure/deployment/containers/docker/Dockerfile
    ports:
      - "3000:3000"
    environment:
      - BENTOML_PORT=3000
      - BENTOML_HOST=0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health_check"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./infrastructure/monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-storage:/var/lib/grafana

volumes:
  grafana-storage:
```

```bash
# å•Ÿå‹•æœå‹™
docker-compose up -d

# æŸ¥çœ‹æœå‹™ç‹€æ…‹
docker-compose ps

# åœæ­¢æœå‹™
docker-compose down
```

---

## â˜ï¸ é›²ç«¯éƒ¨ç½²

### AWS éƒ¨ç½²

#### 1. AWS ECS éƒ¨ç½²

```bash
# å®‰è£ AWS CLI
pip install awscli

# é…ç½® AWS æ†‘è­‰
aws configure

# æ¨é€æ˜ åƒåˆ° ECR
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin your-account.dkr.ecr.us-east-1.amazonaws.com

# æ¨™è¨˜ä¸¦æ¨é€æ˜ åƒ
docker tag iris_classifier:latest your-account.dkr.ecr.us-east-1.amazonaws.com/iris-classifier:latest
docker push your-account.dkr.ecr.us-east-1.amazonaws.com/iris-classifier:latest
```

å‰µå»º ECS ä»»å‹™å®šç¾© `infrastructure/deployment/cloud/aws/task-definition.json`ï¼š

```json
{
  "family": "iris-classifier",
  "networkMode": "awsvpc",
  "requiresCompatibilities": ["FARGATE"],
  "cpu": "512",
  "memory": "1024",
  "executionRoleArn": "arn:aws:iam::your-account:role/ecsTaskExecutionRole",
  "containerDefinitions": [
    {
      "name": "iris-classifier",
      "image": "your-account.dkr.ecr.us-east-1.amazonaws.com/iris-classifier:latest",
      "portMappings": [
        {
          "containerPort": 3000,
          "protocol": "tcp"
        }
      ],
      "essential": true,
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "/ecs/iris-classifier",
          "awslogs-region": "us-east-1",
          "awslogs-stream-prefix": "ecs"
        }
      },
      "healthCheck": {
        "command": [
          "CMD-SHELL",
          "curl -f http://localhost:3000/health_check || exit 1"
        ],
        "interval": 30,
        "timeout": 5,
        "retries": 3,
        "startPeriod": 60
      }
    }
  ]
}
```

#### 2. AWS Lambda éƒ¨ç½²

```bash
# å®‰è£ Serverless Framework
npm install -g serverless

# å‰µå»º serverless.yml
cat > serverless.yml << EOF
service: iris-classifier-lambda

provider:
  name: aws
  runtime: python3.9
  stage: prod
  region: us-east-1

functions:
  predict:
    handler: lambda_handler.predict
    events:
      - http:
          path: predict
          method: post
          cors: true
    timeout: 30
    memorySize: 512

plugins:
  - serverless-python-requirements

custom:
  pythonRequirements:
    dockerizePip: true
EOF

# éƒ¨ç½²
sls deploy
```

### Google Cloud Platform éƒ¨ç½²

#### 1. Cloud Run éƒ¨ç½²

```bash
# å®‰è£ gcloud CLI
# https://cloud.google.com/sdk/docs/install

# èªè­‰
gcloud auth login
gcloud config set project your-project-id

# å»ºç«‹ä¸¦æ¨é€æ˜ åƒ
gcloud builds submit --tag gcr.io/your-project-id/iris-classifier

# éƒ¨ç½²åˆ° Cloud Run
gcloud run deploy iris-classifier \
  --image gcr.io/your-project-id/iris-classifier \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --port 3000 \
  --memory 1Gi \
  --cpu 1
```

#### 2. GKE éƒ¨ç½²

å‰µå»º `infrastructure/deployment/cloud/gcp/deployment.yaml`ï¼š

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: iris-classifier
spec:
  replicas: 3
  selector:
    matchLabels:
      app: iris-classifier
  template:
    metadata:
      labels:
        app: iris-classifier
    spec:
      containers:
      - name: iris-classifier
        image: gcr.io/your-project-id/iris-classifier:latest
        ports:
        - containerPort: 3000
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health_check
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health_check
            port: 3000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: iris-classifier-service
spec:
  selector:
    app: iris-classifier
  ports:
  - port: 80
    targetPort: 3000
  type: LoadBalancer
```

```bash
# éƒ¨ç½²åˆ° GKE
kubectl apply -f infrastructure/deployment/cloud/gcp/deployment.yaml

# æŸ¥çœ‹éƒ¨ç½²ç‹€æ…‹
kubectl get deployments
kubectl get services
kubectl get pods
```

### Azure éƒ¨ç½²

#### 1. Container Instances éƒ¨ç½²

```bash
# å®‰è£ Azure CLI
# https://docs.microsoft.com/en-us/cli/azure/install-azure-cli

# ç™»å…¥ Azure
az login

# å‰µå»ºè³‡æºç¾¤çµ„
az group create --name iris-classifier-rg --location eastus

# éƒ¨ç½²å®¹å™¨
az container create \
  --resource-group iris-classifier-rg \
  --name iris-classifier \
  --image your-registry/iris-classifier:latest \
  --ports 3000 \
  --dns-name-label iris-classifier-unique \
  --memory 1 \
  --cpu 1
```

---

## ğŸ“Š ç›£æ§è¨­ç½®

### 1. Prometheus ç›£æ§

å‰µå»º `infrastructure/monitoring/prometheus/prometheus.yml`ï¼š

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'iris-classifier'
    static_configs:
      - targets: ['localhost:8001']
    metrics_path: '/metrics'
    scrape_interval: 5s

  - job_name: 'bentoml-service'
    static_configs:
      - targets: ['localhost:3000']
    metrics_path: '/metrics'
    scrape_interval: 10s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

å‰µå»ºå‘Šè­¦è¦å‰‡ `infrastructure/monitoring/prometheus/alert_rules.yml`ï¼š

```yaml
groups:
  - name: iris_classifier_alerts
    rules:
      - alert: HighErrorRate
        expr: rate(ml_prediction_errors_total[5m]) > 0.1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors per second"

      - alert: HighLatency
        expr: histogram_quantile(0.95, ml_prediction_duration_seconds) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency detected"
          description: "95th percentile latency is {{ $value }}s"

      - alert: LowAccuracy
        expr: ml_model_accuracy < 0.9
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Model accuracy is low"
          description: "Model accuracy is {{ $value }}"
```

### 2. Grafana å„€è¡¨æ¿

å•Ÿå‹• Grafana ä¸¦å°å…¥å„€è¡¨æ¿ï¼š

```bash
# å•Ÿå‹• Grafana
docker run -d -p 3001:3000 --name grafana grafana/grafana:latest

# è¨ªå• http://localhost:3001
# é è¨­å¸³è™Ÿ: admin/admin

# æ·»åŠ  Prometheus è³‡æ–™æº
# URL: http://localhost:9090
```

å‰µå»ºå„€è¡¨æ¿ JSON é…ç½®ï¼š

```json
{
  "dashboard": {
    "title": "Iris Classifier Monitoring",
    "panels": [
      {
        "title": "Prediction Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(ml_predictions_total[5m])",
            "legendFormat": "{{class}}"
          }
        ]
      },
      {
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, ml_prediction_duration_seconds)",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, ml_prediction_duration_seconds)",
            "legendFormat": "50th percentile"
          }
        ]
      },
      {
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(ml_prediction_errors_total[5m])",
            "legendFormat": "{{error_type}}"
          }
        ]
      },
      {
        "title": "Model Accuracy",
        "type": "singlestat",
        "targets": [
          {
            "expr": "ml_model_accuracy",
            "legendFormat": "Accuracy"
          }
        ]
      }
    ]
  }
}
```

### 3. æ—¥èªŒç›£æ§

ä½¿ç”¨ ELK Stack é€²è¡Œæ—¥èªŒç›£æ§ï¼š

```bash
# å•Ÿå‹• Elasticsearch, Logstash, Kibana
docker-compose -f infrastructure/monitoring/elk/docker-compose.yml up -d
```

ELK `docker-compose.yml`ï¼š

```yaml
version: '3.7'
services:
  elasticsearch:
    image: elasticsearch:7.14.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"

  logstash:
    image: logstash:7.14.0
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    ports:
      - "5000:5000"
    depends_on:
      - elasticsearch

  kibana:
    image: kibana:7.14.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_URL=http://elasticsearch:9200
    depends_on:
      - elasticsearch
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

#### 1. æ¨¡å‹è¼‰å…¥å¤±æ•—

```bash
# æª¢æŸ¥æ¨¡å‹æ–‡ä»¶
ls -la application/registry/model_registry/

# æª¢æŸ¥ MLflow æ¨¡å‹
poetry run python -c "import mlflow; print(mlflow.search_experiments())"

# é‡æ–°è¨“ç·´æ¨¡å‹
poetry run python application/training/pipelines/iris_training_pipeline.py
```

#### 2. æœå‹™ç„¡æ³•å•Ÿå‹•

```bash
# æª¢æŸ¥åŸ è™Ÿå ç”¨
netstat -tulpn | grep :3000

# æª¢æŸ¥ BentoML æœå‹™åˆ—è¡¨
poetry run bentoml list

# æŸ¥çœ‹è©³ç´°éŒ¯èª¤
poetry run bentoml serve iris_service.py:svc --reload --debug
```

#### 3. æ€§èƒ½å•é¡Œ

```bash
# é‹è¡Œæ€§èƒ½æ¸¬è©¦
poetry run python tests/integration/test_load_performance.py --concurrent 1 --requests 10

# æª¢æŸ¥è³‡æºä½¿ç”¨
docker stats iris-service

# èª¿æ•´è³‡æºé…ç½®
# ç·¨è¼¯ bentofile.yaml æˆ– deployment.yaml
```

#### 4. ç›£æ§æŒ‡æ¨™ç¼ºå¤±

```bash
# æª¢æŸ¥ Prometheus ç›®æ¨™
curl http://localhost:9090/targets

# æª¢æŸ¥æŒ‡æ¨™ç«¯é»
curl http://localhost:8001/metrics

# é‡å•Ÿç›£æ§çµ„ä»¶
docker-compose restart prometheus grafana
```

#### 5. BentoML ç‰¹å®šå•é¡Œ

```bash
# æª¢æŸ¥ BentoML ç‰ˆæœ¬å…¼å®¹æ€§
poetry run bentoml --version

# å¸¸è¦‹ API éŒ¯èª¤ä¿®å¾©
# éŒ¯èª¤: AttributeError: module 'bentoml' has no attribute 'service'
# è§£æ±º: æ”¹ç”¨ @bentoml.Service (å¤§å¯« S)

# éŒ¯èª¤: TypeError: Service.__init__() got an unexpected keyword argument 'resources'
# è§£æ±º: å°‡ resources åƒæ•¸ç§»åˆ° @bentoml.api è£é£¾å™¨

# éŒ¯èª¤: Attribute "IrisClassifier" not found in module
# è§£æ±º: æª¢æŸ¥æœå‹™å¯¦ä¾‹åç¨±ï¼Œä½¿ç”¨ iris_service.py:svc

# æŠ‘åˆ¶è­¦å‘Šè¨Šæ¯
PYTHONWARNINGS="ignore" poetry run bentoml serve iris_service.py:svc --reload
```

### 6. Makefile å‘½ä»¤å•é¡Œ

```bash
# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å‘½ä»¤
make help

# æª¢æŸ¥ Poetry ç’°å¢ƒ
poetry env info

# é‡æ–°å®‰è£ä¾è³´
make clean && make install

# æª¢æŸ¥ GPU é…ç½®
make checkgpu

# æ§‹å»ºæœå‹™åŒ…
make bento-build

# å‰µå»ºå®¹å™¨
make containerize

# å•Ÿå‹•æœå‹™
make run
```

**å¸¸è¦‹ Makefile éŒ¯èª¤ï¼š**

#### `make install` å¤±æ•—
```bash
# æª¢æŸ¥ Poetry é–å®šæ–‡ä»¶
ls -la poetry.lock

# é‡æ–°é–å®šä¾è³´
poetry lock --no-update

# æ¸…ç†ç·©å­˜å¾Œé‡æ–°å®‰è£
poetry cache clear --all pypi
make install
```

#### `make run` æœå‹™å•Ÿå‹•å¤±æ•—
```bash
# æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²è¼‰å…¥
poetry run bentoml models list

# æª¢æŸ¥æœå‹™æ§‹å»ºç‹€æ…‹
make bento-build

# æŸ¥çœ‹è©³ç´°æ—¥èªŒ
tail -f bentoml_service.log
```

#### `make containerize` å¤±æ•—
```bash
# æª¢æŸ¥ Docker æ˜¯å¦é‹è¡Œ
docker ps

# æª¢æŸ¥æœå‹™æ˜¯å¦å·²æ§‹å»º
make bento-build

# æŸ¥çœ‹å®¹å™¨æ—¥èªŒ
docker logs <container_id>
```

### é™¤éŒ¯å·¥å…·

#### 1. æ—¥èªŒæŸ¥çœ‹

```bash
# BentoML æœå‹™æ—¥èªŒ
poetry run bentoml logs iris_classifier:latest

# Docker å®¹å™¨æ—¥èªŒ
docker logs iris-service

# Kubernetes Pod æ—¥èªŒ
kubectl logs deployment/iris-classifier
```

#### 2. å¥åº·æª¢æŸ¥

```bash
# æœå‹™å¥åº·æª¢æŸ¥
curl http://localhost:3000/health_check

# è©³ç´°æŒ‡æ¨™
curl http://localhost:3000/get_metrics_summary?hours=1

# Prometheus æŒ‡æ¨™
curl http://localhost:8001/metrics
```

#### 3. æ•ˆèƒ½åˆ†æ

```bash
# è¨˜æ†¶é«”ä½¿ç”¨
poetry run python -c "
import psutil
import os
process = psutil.Process(os.getpid())
print(f'Memory: {process.memory_info().rss / 1024 / 1024:.2f} MB')
"

# CPU ä½¿ç”¨
top -p $(pgrep -f bentoml)
```

---

## ğŸ¯ æœ€ä½³å¯¦è¸

### 1. å®‰å…¨æ€§

- ä½¿ç”¨ HTTPS
- å¯¦æ–½ API é‡‘é‘°èªè­‰
- å®šæœŸæ›´æ–°ä¾è³´é …
- æƒæå®¹å™¨æ¼æ´

### 2. å¯æ“´å±•æ€§

- ä½¿ç”¨è² è¼‰å‡è¡¡å™¨
- å¯¦æ–½æ°´å¹³æ“´å±•
- è¨­ç½®è‡ªå‹•æ“´å±•
- ç›£æ§è³‡æºä½¿ç”¨

### 3. å¯é æ€§

- å¯¦æ–½å¥åº·æª¢æŸ¥
- è¨­ç½®é‡è©¦æ©Ÿåˆ¶
- é…ç½®ç›£æ§å‘Šè­¦
- å®šæœŸå‚™ä»½æ¨¡å‹

### 4. æ•ˆèƒ½å„ªåŒ–

- ä½¿ç”¨æ¨¡å‹é‡åŒ–
- å¯¦æ–½æ‰¹æ¬¡è™•ç†
- èª¿æ•´ä¸¦ç™¼è¨­ç½®
- ä½¿ç”¨å¿«å–æ©Ÿåˆ¶

---

## ğŸ“ æ”¯æ´

é‡åˆ°å•é¡Œï¼Ÿ

1. æŸ¥çœ‹ [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤) éƒ¨åˆ†
2. æª¢æŸ¥å°ˆæ¡ˆ [README.md](README.md)
3. æŸ¥é–± [MLOps å®Œæ•´æ•™å­¸](MLOPS_COMPLETE_TUTORIAL.md)
4. æäº¤ Issue åˆ°å°ˆæ¡ˆå„²å­˜åº«

**æ­å–œï¼æ‚¨ç¾åœ¨æ“æœ‰å®Œæ•´çš„ MLOps éƒ¨ç½²å’Œç›£æ§ç³»çµ±ï¼** ğŸ‰